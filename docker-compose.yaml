version: '3.8'

# ------------------- Сети -------------------
networks:
  etl-network:
    driver: bridge

# ------------------- Тома -------------------
volumes:
  minio-data:
  postgres-data:
  clickhouse-data:

services:
  # ---------- MinIO (S3-совместимое хранилище) ----------
  minio:
    image: minio/minio
    container_name: minio
    networks:
      - etl-network
    volumes:
      - minio-data:/data
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Web UI
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    restart: unless-stopped

  # ---------- Spark Master ----------
  spark-master:
    image: bitnami/spark:3.3.1
    platform: linux/amd64
    container_name: spark-master
    networks:
      - etl-network
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
      SPARK_DAEMON_JAVA_OPTS: >-
        -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
        -Dspark.hadoop.fs.s3a.endpoint=http://minio:9000
        -Dspark.hadoop.fs.s3a.access.key=minioadmin
        -Dspark.hadoop.fs.s3a.secret.key=minioadmin
        -Dspark.hadoop.fs.s3a.path.style.access=true
        -Dspark.hadoop.fs.s3a.connection.ssl.enabled=false
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./spark/scripts/process_raw_trn.py:/opt/bitnami/spark/apps/process_raw_trn.py:ro
    deploy:
      resources:
        limits:
          memory: 4g
          cpus: '1.0'

  # ---------- Spark Worker ----------
  spark-worker:
    image: bitnami/spark:3.3.1
    platform: linux/amd64
    container_name: spark-worker
    networks:
      - etl-network
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_DAEMON_JAVA_OPTS: >-
        -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
        -Dspark.hadoop.fs.s3a.endpoint=http://minio:9000
        -Dspark.hadoop.fs.s3a.access.key=minioadmin
        -Dspark.hadoop.fs.s3a.secret.key=minioadmin
        -Dspark.hadoop.fs.s3a.path.style.access=true
        -Dspark.hadoop.fs.s3a.connection.ssl.enabled=false
    deploy:
      resources:
        limits:
          memory: 3g
          cpus: '1.0'

  # ---------- PostgreSQL для Airflow метаданных ----------
  postgres:
    image: postgres:13
    container_name: airflow-postgres
    networks:
      - etl-network
    restart: unless-stopped
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data


  # ---------- Инициализация Airflow ----------
  airflow-init:
    image: apache/airflow:2.6.3
    container_name: airflow-init
    networks:
      - etl-network
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: '8RsawjB0bMwBpoBxyWsxo8qR0aEMd0k724GXchilCO4='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    entrypoint: >
      bash -c "airflow db init &&
                airflow users create \
                  --username admin \
                  --password admin \
                  --firstname Airflow \
                  --lastname Admin \
                  --role Admin \
                  --email admin@example.com"
    restart: 'no'

  # ---------- Airflow Webserver (с поддержкой SparkSubmitOperator) ----------
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: my-airflow:2.6.3-spark
    container_name: airflow-webserver
    networks:
      - etl-network
    depends_on:
      - postgres
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: '8RsawjB0bMwBpoBxyWsxo8qR0aEMd0k724GXchilCO4='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__RBAC: 'true'
      # автоконнект spark_default
      AIRFLOW_CONN_SPARK_DEFAULT: "spark://spark-master:7077"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      # сюда монтируем ваш sparс-скрипт
      - ./spark/scripts:/opt/airflow/spark/scripts:ro
    ports:
      - "8081:8080"
    command: webserver
    restart: unless-stopped

  # ---------- Airflow Scheduler (с поддержкой SparkSubmitOperator) ----------
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: my-airflow:2.6.3-spark
    container_name: airflow-scheduler
    networks:
      - etl-network
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: '8RsawjB0bMwBpoBxyWsxo8qR0aEMd0k724GXchilCO4='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      # автоконнект spark_default
      AIRFLOW_CONN_SPARK_DEFAULT: "spark://spark-master:7077"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      # сюда монтируем ваш sparс-скрипт
      - ./spark/scripts:/opt/airflow/spark/scripts:ro
    command: scheduler
    restart: unless-stopped

  # ---------- ClickHouse для чтения Parquet через S3 ----------
  clickhouse:
    image: yandex/clickhouse-server:latest
    platform: linux/amd64
    container_name: clickhouse
    networks:
      - etl-network
    environment:
      CLICKHOUSE_DB: default
    ports:
      - "8123:8123"    # HTTP
      - "8124:9000"    # TCP
    volumes:
      - clickhouse-data:/var/lib/clickhouse
